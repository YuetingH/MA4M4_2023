{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5cdaf388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# This will filter warnings, which can be lengthy but sometimes useful. You may comment it out if you want to see them.\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81ad0ba3",
   "metadata": {},
   "source": [
    "# Construct Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5bcddc6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.loadtxt('Data/data1.dat', dtype='int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fd039cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_dataset(data):\n",
    "    '''\n",
    "    Input: \n",
    "        data: the original dataset (12 days)\n",
    "    Output: \n",
    "        data_list: a list of three partitioned datasets (4 days each)\n",
    "    '''\n",
    "    \n",
    "    data1, data2, data3 = [], [], []\n",
    "    time_division = [60*60*24 * 4, 60*60*24 * 8]\n",
    "    \n",
    "    for contact in data:\n",
    "        contact = np.array(contact)\n",
    "        if contact[0] < time_division[0]:\n",
    "            data1.append(contact)\n",
    "        elif contact[0] < time_division[1]:\n",
    "            data2.append(contact)\n",
    "        else:\n",
    "            data3.append(contact)\n",
    "    \n",
    "    data_list = [np.array(data1), np.array(data2), np.array(data3)]\n",
    "    \n",
    "    return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c06337e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_list = divide_dataset(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "18dc178b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def process_dataset(data_list):\n",
    "    '''\n",
    "    Input: \n",
    "        data_list: a list of three partitioned datasets\n",
    "    Output: \n",
    "        data_tuple: a list of three preprocessed datasets in the form of tuple: (ID1, ID2, weight). \n",
    "    '''\n",
    "    \n",
    "    data_tuple = []\n",
    "    \n",
    "    for data_p in data_list:\n",
    "        pairs = list(zip(data_p[:, 1],data_p[:, 2])) # create a list of pairs: (ID1, ID2) to represent each contact\n",
    "        dict_pairs = dict(Counter(pairs)) # count the occurance of each pair; in the form of dictionary: \"(ID1, ID2): count\"\n",
    "        tuple_pairs = [(k[0], k[1], dict_pairs[k]) for k in dict_pairs.keys()] # reorganise into tuples\n",
    "        \n",
    "        data_tuple.append(tuple_pairs)\n",
    "    \n",
    "    return data_tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb132484",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tuple = process_dataset(data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d272878f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_network(data_tuple):\n",
    "    '''\n",
    "    Input: \n",
    "        data_tuple: a list of three preprocessed datasets in the form of tuple: (ID1, ID2, weight).\n",
    "    Output: \n",
    "        G_list: a list of three undirected weighted networks.\n",
    "    '''\n",
    "    \n",
    "    G_list = []\n",
    "    \n",
    "    for data_tuple_p in data_tuple:\n",
    "        G_p = nx.Graph()\n",
    "        individuals = np.concatenate((data[:, 1], data[:, 2])) # individuals involved in two columns\n",
    "        G_p.add_nodes_from(set(individuals)) # Each individual is represented as a node in a graph\n",
    "        G_p.add_weighted_edges_from(data_tuple_p)\n",
    "        \n",
    "        G_list.append(G_p)\n",
    "    \n",
    "    return G_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8542abfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "G_list = get_network(data_tuple)\n",
    "G1, G2, G3 = G_list"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7281697a",
   "metadata": {},
   "source": [
    "# Add Department of Each Individual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f7ebed52",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_department = np.loadtxt('Data/data2.txt', dtype = str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "04731268",
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_department = {int(row[0]):row[1] for row in data_department}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "498e5a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for G in G_list: \n",
    "    nx.set_node_attributes(G, dict_department, \"department\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633bf807",
   "metadata": {},
   "source": [
    "# Community Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "0c2a0004",
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx.algorithms.community as nx_comm\n",
    "\n",
    "def G_comm_dict(G):\n",
    "    \n",
    "    # Community detection\n",
    "    comm = nx_comm.louvain_communities(G, resolution = 0.5, seed = 0)\n",
    "    \n",
    "    # Arrange the sets in the comm list in a descending order based on the size of each set\n",
    "    comm_copy = comm.copy(); # retain the input \"comm\"\n",
    "    comm_copy.sort(key=len, reverse = True) \n",
    "    \n",
    "    # Create dict\n",
    "    comm_dict = {}\n",
    "    index = 0\n",
    "    for comm_set in comm_copy:\n",
    "        for node in comm_set:\n",
    "            comm_dict.update({node: index})\n",
    "        index += 1\n",
    "    \n",
    "    # Add node attribute\n",
    "    nx.set_node_attributes(G, comm_dict, \"comm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "11f8ed75",
   "metadata": {},
   "outputs": [],
   "source": [
    "for G in G_list: \n",
    "    G_comm_dict(G)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7d536b6",
   "metadata": {},
   "source": [
    "# Save Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "16f3ac30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'department': 'DSE', 'comm': 0}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G1.nodes[513]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f9f08d6c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'department': 'DSE', 'comm': 4}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G2.nodes[513]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8978a641",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'department': 'DSE', 'comm': 0}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G3.nodes[513]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "10ae2265",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7}"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(nx.get_node_attributes(G1, 'comm').values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f1ddd774",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(nx.get_node_attributes(G2, 'comm').values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "053e29c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12}"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(nx.get_node_attributes(G3, 'comm').values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e887294f",
   "metadata": {},
   "outputs": [],
   "source": [
    "nx.write_gpickle(G1, \"Data/G1.gpickle\")\n",
    "nx.write_gpickle(G2, \"Data/G2.gpickle\")\n",
    "nx.write_gpickle(G3, \"Data/G3.gpickle\")"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
